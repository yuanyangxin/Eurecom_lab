{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>MALIS Lab Session 2 - Fall 2018</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this lab is to practice with Support Vector Machines (SVM), and in particular with the PEGASOS (Primal Estimated sub-GrAdient SOlver for SVM) algorithm. PEGASOS is a fast stochastic sub-gradient descent algorithm for solving the primal optimization problem cast by an SVM. In the first part of this lab, you will be to implement PEGASOS. As it is based on a primal problem, it is only adapted to linear kernels; the authors of PEGASOS proposed a way to use other non-linear kernels, at the price of time efficiency. Therefore in the second part of the lab, instead of implementing that less efficient version of PEGASOS, you will work on a features mapping method that will allow you to separate non-linearly separable datasets using PEGASOS. You will test your implementations on four datasets.\n",
    "\n",
    "Experiments should be made by groups of two students. Each group should produce a Jupyter Notebook with all their results and comments. We strongly encourage the addition of plots and visual representations to the report, bearing in mind that comments on graphical data are still necessary. Code for adding images to your notebook: ```<img src=\"path/to/image.png\" />```.\n",
    "\n",
    "Submit your complete notebook as an archive (tar -cf groupXnotebook.tar lab2/). Deadline for submitting your notebook: 21 December 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Introduction</h2>\n",
    "There are three parts to this lab session. \n",
    "\n",
    "1. A part on PEGASOS where you are asked to implement the stochastic version of PEGASOS;\n",
    "\n",
    "2. A part on features mapping.\n",
    "\n",
    "Data are 2D. You can visualize them by running the following cell (red dots are labeled -1 and green dots are labeled 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import print_dataset\n",
    "print_dataset(\"data/dataset1.txt\")\n",
    "print_dataset(\"data/dataset2.txt\")\n",
    "print_dataset(\"data/dataset3.txt\")\n",
    "print_dataset(\"data/dataset4.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Question 1</h4>\n",
    "According to the above plots, do these datasets seem to be linearly separable? On which of these datasets do you expect a linear SVM to perform well? On which ones do you think it will perform badly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Your answer</h4>\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 1: PEGASOS</h2>\n",
    "\n",
    "<h3>How PEGASOS works</h3>\n",
    "\n",
    "<i>That sub-part gives some intuition about how PEGASOS can actually find the solution of a problem cast by a linear SVM. Reading it is not required to do the lab, but we encourage you to have a look at it as it may help you for your implementation.</i>\n",
    "\n",
    "Let us assume that given a set $S = \\lbrace (x_i, y_i) \\rbrace$ where $x_i \\in \\mathbb{R}^n$ and $y_i \\in \\lbrace -1, 1 \\rbrace$, we want to find a solution of the following problem\n",
    "\n",
    "$$\\min_{w, b} \\frac{1}{2} \\Vert w \\Vert^2 + C \\sum_i \\max(0, 1-y_i(w^Tx_i + b))$$\n",
    "\n",
    "where $C$ is a non-zero hyperparameter controlling the \"softness\" of the margin (the lower the $C$ the softer the margin), or equivalently\n",
    "\n",
    "$$\\min_{w, b} \\frac{\\lambda}{2} \\Vert w \\Vert^2 + \\sum_i \\max(0, 1-y_i(w^Tx_i + b))$$\n",
    "\n",
    "where $\\lambda$ replaces $C$ (the higher the $\\lambda$ the softer the margin). We can get rid of the bias term $b$ by adding a constant feature to all the $x_i$ (we can assume without loss of generality that the value of the constant is 1) and by replacing $w$ by the concatenation of $w$ and $b$. In that setup, the optimization problem becomes (with $\\tilde{w} = [w; b]$ and $\\tilde{x_i} = [x_i; 1]$)\n",
    "\n",
    "$$\\min_{\\tilde{w}} \\frac{\\lambda}{2} \\Vert \\tilde{w} \\Vert^2 + \\sum_i \\max(0, 1-y_i(\\tilde{w}^T\\tilde{x_i})).$$\n",
    "\n",
    "You can notice that the optimization problem we obtained is slightly different from the one we stated before, as $\\Vert \\tilde{w} \\Vert^2$ should be replaced by $\\Vert \\tilde{w} \\Vert^2 - b^2$. However, in practice, that difference has very few importance. Therefore in the following we will assume that $x_i \\in \\mathbb{R}^{n+1}$, $w \\in \\mathbb{R}^{n+1}$, and that the last feature of $x_i$ is always equal to 1. As we will work on the stochastic variant of PEGASOS, we will consider the following objective:\n",
    "\n",
    "$$\\min_{w_t} \\frac{\\lambda}{2} \\Vert w_t \\Vert^2 + \\max(0, 1-y_{i_t}(w_t^Tx_{i_t}))$$\n",
    "\n",
    "where $(x_{i_t}, y_{i_t})$ is a sample drawn randomly at iteration $t$ from $S$. The sub-gradient of the above objective is then given by:\n",
    "\n",
    "$$\\nabla w_t = \\lambda w_t - \\phi_I(x_{i_t}, y_{i_t}) y_{i_t}x_{i_t}$$\n",
    "\n",
    "where $\\phi_I$ is the indicator function of the set $I = \\lbrace (x, y) \\in R^d \\times \\lbrace -1, 1 \\rbrace : y w^Tx < 1 \\rbrace$ ($\\phi_I(x, y) = 1$ if $(x, y) \\in I$ and $\\phi_I(x, y) = 0$ otherwise). Then, we compute $w_{t+1}$ using the following update formula:\n",
    "\n",
    "$$w_{t+1} = w_t - \\eta_t \\nabla w_t$$\n",
    "\n",
    "where $\\eta_t = 1/(\\lambda t)$. The algorithm stops after a predefined number of iterations. As you can understand, PEGASOS uses a stochastic gradient descent with learning rate $\\eta_t$ to find the optimal parameters of an SVM. The next sub-part gives you a pseudo-code of PEGASOS. Your role will be to implement it based on that pseudo-code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Pseudo-code of PEGASOS</h3>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "INPUTS: S = {(x_1, y_1), ..., (x_n, y_n)}   # Dataset (x_i are data vectors and y_i are data labels in {-1, 1})\n",
    "        lambda   # Predefined hyperparameter\n",
    "        T   # Total number of iterations\n",
    "\n",
    "PEGASOS(S, lambda, T):\n",
    "    w_1 = [0, ..., 0]\n",
    "    For t in {1, ..., n}:\n",
    "        Pick (x, y) in S uniformly at random\n",
    "        Set eta_t = 1/(lambda*t)\n",
    "        If y*dot_product(w_t, x) < 1:\n",
    "            Set w_{t+1} = w_t - eta_t*(lambda*w_t - y*x)\n",
    "        Else:\n",
    "            Set w_{t+1} = w_t - eta_t*lambda*w_t\n",
    "    Return w_T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task will now be to implement PEGASOS. An SVM class has been defined in utils.py. You are strongly encouraged to read it carefully to understand it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Question 2</h4>\n",
    "Complete the following implementation of PEGASOS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Your answer</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import SVM\n",
    "def train_one_iteration(self, learning_rate, eta_t):\n",
    "    '''\n",
    "    Train for one epoch according to PEGASOS\n",
    "    '''\n",
    "    self.setnextinput() # Picks (x, y) in dataset uniformly at random\n",
    "    if ### IMPLEMENTATION REQUIRED ###\n",
    "        new_w = ### IMPLEMENTATION REQUIRED ###\n",
    "    else:\n",
    "        new_w = ### IMPLEMENTATION REQUIRED ###\n",
    "    self.w = new_w\n",
    "\n",
    "SVM.train_one_iteration = train_one_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Question 3</h4>\n",
    "Test your implementation on the four datasets (train your SVM during 1500 iterations with $\\lambda = 0.0001$). Do you find the results that you forecast in Question 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Your answer</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Dataset\n",
    "\n",
    "# Tests on dataset1\n",
    "datafile = \"data/dataset1_train.txt\"\n",
    "data = Dataset(datafile)\n",
    "test_datafile = \"data/dataset1_test.txt\"\n",
    "test_data = Dataset(test_datafile)\n",
    "\n",
    "svm = SVM(data, test_data)\n",
    "svm.train(1500, 0.0001)\n",
    "svm.make_plot() # Train accuracy in red, test accuracy in green\n",
    "svm.print_accuracy()\n",
    "\n",
    "# Tests on other datasets\n",
    "### IMPLEMENTATION REQUIRED ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your comments in this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 2: Features Maps</h2>\n",
    "\n",
    "As you know, it is possible to use the \"Kernel Trick\" to separate non-linearly separable datasets with SVMs. This trick is meant to be applied on the dual problem. As PEGASOS is based on the primal problem, the Kernel Trick cannot be used easily with PEGASOS. However, there is a method to approximate the feature space corresponding to a given kernel. In this part, you will implement and test that method. You will not be asked to dig into the mathematical theory behind that method, but if you wish, you can find more informations in the paper <i>Random Features for Large-Scale Kernel Machines</i> by Ali Rahimi and Ben Recht [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Approximating the Radial Basis Function (RBF) kernel</h2>\n",
    "\n",
    "The Radial Basis Function $K_\\gamma$ is one of the most common kernels used with SVMs. It is defined as follows ($x, y \\in \\mathbb{R}^d$):\n",
    "\n",
    "$$ K_\\gamma(x, y) = \\exp(-\\gamma \\Vert x - y \\Vert^2).$$\n",
    "\n",
    "Its feature space is of infinite dimension. However, you can approximate it according to the following algorithm (details in [1]):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "INPUTS:\n",
    "    S = {(x1, y1), ..., (xn, yn)}   # Dataset\n",
    "    mean = zeros(d)\n",
    "    cov = 2*d*gamma*eye(d)\n",
    "    D = wanted dimension of the approximation of the RBF feature space\n",
    "\n",
    "FEATURES_MAP(S, mean, sigma, D):\n",
    "    S' = {}\n",
    "    p ~ multivariate_normal_distribution(mean, cov)\n",
    "    Draw D iid samples w1, ..., wD from p and D iid samples b1, ..., bD uniformly in [0, 2*pi]\n",
    "    For all x in S:\n",
    "        z = [cos(dot_product(w1, x) + b1), ..., cos(dot_product(wD, x) + bD)]\n",
    "        z *= sqrt(2/D)\n",
    "        Add z to S'\n",
    "    Return S'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pseudo-code above transforms data from its original space to a new feature space approximating the feature space corresponding to the RBF kernel. You will now implement that pseudo-code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Question 4</h4>\n",
    "Complete the following implementation of the FeaturesMap class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Your answer</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import Dataset, PI\n",
    "from math import cos\n",
    "import random\n",
    "\n",
    "class FeaturesMap:\n",
    "\n",
    "    def __init__(self, dim, old_dim, gamma):\n",
    "        mean = ### IMPLEMENTATION REQUIRED ###\n",
    "        cov = ### IMPLEMENTATION REQUIRED ###\n",
    "        self.dim = dim\n",
    "        self.omega = [np.random.multivariate_normal(mean, cov) for _ in range(dim)]\n",
    "        self.b = [random.random()*2*PI for _ in range(dim)]\n",
    "\n",
    "    def __call__(self, dataset):\n",
    "        outputs = dataset.output\n",
    "        inputs = []\n",
    "        for x in dataset.input:\n",
    "            new_feats = []\n",
    "            for i in range(self.dim):\n",
    "                feat = ### IMPLEMENTATION REQUIRED ###\n",
    "                new_feats.append(feat)\n",
    "            new_feats = np.array(new_feats)\n",
    "            new_feats *= np.sqrt(2/self.dim)\n",
    "            inputs.append(new_feats)\n",
    "        new_data = Dataset(input_size=self.dim, length=dataset.len)\n",
    "        new_data.input = inputs\n",
    "        new_data.output = outputs\n",
    "        return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Question 5</h4>\n",
    "Test your implementation on the four artificial datasets. Train your SVM during 1500 iterations with $\\lambda = 0.0001$. The dimension of the feature space is 200 and $\\gamma = 1$. How do your results change with respect to what you found in Question 3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Dataset\n",
    "\n",
    "# Tests on dataset1\n",
    "datafile = \"data/dataset1_train.txt\"\n",
    "data = Dataset(datafile)\n",
    "test_datafile = \"data/dataset1_test.txt\"\n",
    "test_data = Dataset(test_datafile)\n",
    "\n",
    "features_map = FeaturesMap(200, 2, 1)\n",
    "new_data = features_map(data)\n",
    "new_test_data = features_map(test_data)\n",
    "\n",
    "svm = SVM(new_data, new_test_data)\n",
    "svm.train(1500, 0.0001)\n",
    "svm.make_plot() # Train accuracy in red, test accuracy in green\n",
    "svm.print_accuracy()\n",
    "\n",
    "# Tests on other datasets\n",
    "### IMPLEMENTATION REQUIRED ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your comments in this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>The role of $\\gamma$ in the RBF kernel</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Question 6</h4>\n",
    "Do the same tests with $\\gamma = 10^{-6}$. What do you notice? How can you explain your results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Dataset\n",
    "\n",
    "# Tests on dataset1\n",
    "datafile = \"data/dataset1_train.txt\"\n",
    "data = Dataset(datafile)\n",
    "test_datafile = \"data/dataset1_test.txt\"\n",
    "test_data = Dataset(test_datafile)\n",
    "\n",
    "features_map = FeaturesMap(200, 2, 1e-6)\n",
    "new_data = features_map(data)\n",
    "new_test_data = features_map(test_data)\n",
    "\n",
    "svm = SVM(new_data, new_test_data)\n",
    "svm.train(1500, 0.0001)\n",
    "svm.make_plot() # Train accuracy in red, test accuracy in green\n",
    "svm.print_accuracy()\n",
    "\n",
    "# Tests on other datasets\n",
    "### IMPLEMENTATION REQUIRED ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your comments in this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Question 7</h4>\n",
    "Do the same tests with $\\gamma = 10^{4}$. What do you notice? How can you explain your results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Dataset\n",
    "\n",
    "# Tests on dataset1\n",
    "datafile = \"data/dataset1_train.txt\"\n",
    "data = Dataset(datafile)\n",
    "test_datafile = \"data/dataset1_test.txt\"\n",
    "test_data = Dataset(test_datafile)\n",
    "\n",
    "features_map = FeaturesMap(200, 2, 1e4)\n",
    "new_data = features_map(data)\n",
    "new_test_data = features_map(test_data)\n",
    "\n",
    "svm = SVM(new_data, new_test_data)\n",
    "svm.train(1500, 0.0001)\n",
    "svm.make_plot() # Train accuracy in red, test accuracy in green\n",
    "svm.print_accuracy()\n",
    "\n",
    "# Tests on other datasets\n",
    "### IMPLEMENTATION REQUIRED ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your comments in this cell."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
